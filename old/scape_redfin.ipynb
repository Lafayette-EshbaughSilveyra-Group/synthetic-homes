{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dbcb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.13/site-packages (4.67.1)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp)\n",
      "  Downloading frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (17 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp)\n",
      "  Downloading multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp)\n",
      "  Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp)\n",
      "  Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading aiohttp-3.12.11-cp313-cp313-macosx_11_0_arm64.whl (464 kB)\n",
      "Downloading multidict-6.4.4-cp313-cp313-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading yarl-1.20.0-cp313-cp313-macosx_11_0_arm64.whl (94 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.6.2-cp313-cp313-macosx_11_0_arm64.whl (48 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading propcache-0.3.1-cp313-cp313-macosx_11_0_arm64.whl (44 kB)\n",
      "Installing collected packages: propcache, multidict, idna, frozenlist, attrs, aiohappyeyeballs, yarl, aiosignal, aiohttp\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9/9\u001b[0m [aiohttp]5;237m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8/9\u001b[0m [aiohttp]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.11 aiosignal-1.3.2 attrs-25.3.0 frozenlist-1.6.2 idna-3.10 multidict-6.4.4 propcache-0.3.1 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q playwright\n",
    "!playwright install chromium\n",
    "!pip install tqdm\n",
    "!pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Scraping Bethlehem (max 5)...\n",
      "üîé Page 1: found 41 cards\n",
      "‚úÖ Bethlehem: Collected 5 listings.\n",
      "\n",
      "üîç Scraping Easton (max 5)...\n",
      "üîé Page 1: found 41 cards\n",
      "‚úÖ Easton: Collected 5 listings.\n",
      "\n",
      "üîç Scraping Allentown (max 5)...\n",
      "üîé Page 1: found 41 cards\n",
      "‚úÖ Allentown: Collected 5 listings.\n",
      "\n",
      "üéâ Saved 15 total listings to tri_city_listings.json\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import aiohttp\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "BASE_URL = \"https://www.redfin.com\"\n",
    "CITIES = {\n",
    "    \"Bethlehem\": \"1616\",\n",
    "    \"Easton\": \"5583\",\n",
    "    \"Allentown\": \"514\"\n",
    "}\n",
    "MAX_LISTINGS_PER_CITY = 5\n",
    "\n",
    "\n",
    "def sanitize_filename(text):\n",
    "    return text.replace(\" \", \"-\").replace(\",\", \"\").replace(\"/\", \"-\")\n",
    "\n",
    "\n",
    "async def download_images(image_urls, address, city, session, out_dir=\"images\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    saved_paths = []\n",
    "\n",
    "    for idx, url in enumerate(image_urls):\n",
    "        ext = os.path.splitext(urlparse(url).path)[1] or \".jpg\"\n",
    "        safe_address = sanitize_filename(address)\n",
    "        filename = f\"{city}_{safe_address}_{idx}{ext}\"\n",
    "        filepath = os.path.join(out_dir, filename)\n",
    "\n",
    "        try:\n",
    "            async with session.get(url) as resp:\n",
    "                if resp.status == 200:\n",
    "                    with open(filepath, \"wb\") as f:\n",
    "                        f.write(await resp.read())\n",
    "                    saved_paths.append(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to download {url}: {e}\")\n",
    "\n",
    "    return saved_paths\n",
    "\n",
    "\n",
    "async def add_stealth(page):\n",
    "    await page.add_init_script(\"\"\"\n",
    "    Object.defineProperty(navigator, 'webdriver', { get: () => undefined });\n",
    "    window.chrome = { runtime: {} };\n",
    "    Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });\n",
    "    Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "async def scrape_listing(page, url, city_name, session):\n",
    "    try:\n",
    "        await page.goto(url, timeout=30000)\n",
    "        await page.wait_for_timeout(1500)\n",
    "\n",
    "        address = await page.text_content(\"div.street-address\")\n",
    "        desc_tag = await page.query_selector('div[data-rf-test-id=\"listingRemarks\"]')\n",
    "        description = await desc_tag.inner_text() if desc_tag else \"\"\n",
    "\n",
    "        # Stats (beds, baths, sqft)\n",
    "        beds_el = await page.query_selector('div[data-rf-test-id=\"abp-beds\"] .statsValue')\n",
    "        baths_el = await page.query_selector('div[data-rf-test-id=\"abp-baths\"] .statsValue')\n",
    "        sqft_el = await page.query_selector('div[data-rf-test-id=\"abp-sqFt\"] .statsValue')\n",
    "\n",
    "        beds = await beds_el.inner_text() if beds_el else None\n",
    "        baths = await baths_el.inner_text() if baths_el else None\n",
    "        sqft = await sqft_el.inner_text() if sqft_el else None\n",
    "\n",
    "        # Key Details\n",
    "        key_rows = await page.query_selector_all(\"div.KeyDetailsTable div.keyDetails-row\")\n",
    "        year_built = property_type = price_per_sqft = None\n",
    "\n",
    "        for row in key_rows:\n",
    "            label_el = await row.query_selector(\"span.valueType\")\n",
    "            value_el = await row.query_selector(\"span.valueText\")\n",
    "            if not label_el or not value_el:\n",
    "                continue\n",
    "            label = (await label_el.inner_text()).strip()\n",
    "            value = (await value_el.inner_text()).strip()\n",
    "\n",
    "            if label == \"Year Built\":\n",
    "                year_built = value\n",
    "            elif label == \"Property Type\":\n",
    "                property_type = value\n",
    "            elif label == \"Price/Sq.Ft.\":\n",
    "                price_per_sqft = value\n",
    "\n",
    "        # Image scraping\n",
    "        img_tags = await page.query_selector_all(\"a[data-rf-test-id^='MB-image-card-'] img\")\n",
    "        image_urls = []\n",
    "        for img in img_tags:\n",
    "            src = await img.get_attribute(\"src\")\n",
    "            if src:\n",
    "                image_urls.append(src)\n",
    "\n",
    "        local_image_paths = await download_images(image_urls, address or \"unknown\", city_name, session)\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"address\": address.strip() if address else \"\",\n",
    "            \"description\": description.strip(),\n",
    "            \"beds\": beds,\n",
    "            \"baths\": baths,\n",
    "            \"sqft\": sqft,\n",
    "            \"year_built\": year_built,\n",
    "            \"property_type\": property_type,\n",
    "            \"price_per_sqft\": price_per_sqft,\n",
    "            \"image_paths\": local_image_paths\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "async def scrape_city(page, city_name, city_id, max_listings, session):\n",
    "    print(f\"\\nüîç Scraping {city_name} (max {max_listings})...\")\n",
    "    listings = []\n",
    "    page_num = 1\n",
    "\n",
    "    while len(listings) < max_listings:\n",
    "        url = f\"{BASE_URL}/city/{city_id}/PA/{city_name}/page-{page_num}\"\n",
    "        for attempt in range(2):\n",
    "            try:\n",
    "                await page.goto(url, timeout=60000)\n",
    "                break\n",
    "            except TimeoutError:\n",
    "                print(f\"‚ö†Ô∏è Timeout on {url}, retrying ({attempt+1}/2)...\")\n",
    "                await page.wait_for_timeout(3000)\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to load {url} after 2 attempts.\")\n",
    "            break\n",
    "\n",
    "        await page.wait_for_timeout(2000)\n",
    "        await page.evaluate(\"() => window.scrollTo(0, document.body.scrollHeight)\")\n",
    "        await page.wait_for_timeout(2000)\n",
    "\n",
    "        try:\n",
    "            await page.wait_for_selector(\"div.MapHomeCard\", timeout=15000)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Timeout waiting for listings on {city_name} page {page_num}: {e}\")\n",
    "            break\n",
    "\n",
    "        cards = await page.query_selector_all(\"div.MapHomeCard a[href*='/PA/']\")\n",
    "        print(f\"üîé Page {page_num}: found {len(cards)} cards\")\n",
    "\n",
    "        if not cards:\n",
    "            html = await page.content()\n",
    "            with open(f\"debug_{city_name.lower()}_page{page_num}.html\", \"w\") as f:\n",
    "                f.write(html)\n",
    "            break\n",
    "\n",
    "        hrefs = []\n",
    "        for card in cards:\n",
    "            href = await card.get_attribute(\"href\")\n",
    "            if href and (BASE_URL + href) not in hrefs:\n",
    "                hrefs.append(BASE_URL + href)\n",
    "\n",
    "        for href in hrefs:\n",
    "            if len(listings) >= max_listings:\n",
    "                break\n",
    "            data = await scrape_listing(page, href, city_name, session)\n",
    "            if data:\n",
    "                data[\"city\"] = city_name\n",
    "                listings.append(data)\n",
    "\n",
    "        page_num += 1\n",
    "\n",
    "    print(f\"‚úÖ {city_name}: Collected {len(listings)} listings.\")\n",
    "    return listings\n",
    "\n",
    "\n",
    "async def main():\n",
    "    all_data = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=False,\n",
    "            args=[\"--no-sandbox\", \"--disable-blink-features=AutomationControlled\"]\n",
    "        )\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36\",\n",
    "            viewport={\"width\": 1280, \"height\": 800},\n",
    "            locale=\"en-US\",\n",
    "            extra_http_headers={\"Accept-Language\": \"en-US,en;q=0.9\"}\n",
    "        )\n",
    "        page = await context.new_page()\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            for city, city_id in CITIES.items():\n",
    "                listings = await scrape_city(page, city, city_id, MAX_LISTINGS_PER_CITY, session)\n",
    "                all_data.extend(listings)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    with open(\"tri_city_listings.json\", \"w\") as f:\n",
    "        json.dump(all_data, f, indent=2)\n",
    "    print(f\"\\nüéâ Saved {len(all_data)} total listings to tri_city_listings.json\")\n",
    "\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
